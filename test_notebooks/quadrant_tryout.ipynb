{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd19d5cb-d2dc-4224-9bcc-c3a3b215a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install qdrant-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bde9b8-eee0-4293-8a94-473e3b647f6e",
   "metadata": {},
   "source": [
    "## Generatring  splade vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0234fa09-7288-4d31-85f8-41217a584d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38261cd3-911a-4ddb-b27f-5ecc38c76344",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Arthur Robert Ashe Jr. (July 10, 1943 â€“ February 6, 1993) was an American professional tennis player. He won three Grand Slam titles in singles and two in doubles.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6991d8-ae87-47eb-b1b3-9035b4c6a5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_vector(text):\n",
    "    \"\"\"\n",
    "    Computes a vector from logits and attention mask using ReLU, log, and max operations.\n",
    "\n",
    "    Args:\n",
    "    logits (torch.Tensor): The logits output from a model.\n",
    "    attention_mask (torch.Tensor): The attention mask corresponding to the input tokens.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Computed vector.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    output = model(**tokens)\n",
    "    logits, attention_mask = output.logits, tokens.attention_mask\n",
    "    relu_log = torch.log(1 + torch.relu(logits))\n",
    "    weighted_log = relu_log * attention_mask.unsqueeze(-1)\n",
    "    max_val, _ = torch.max(weighted_log, dim=1)\n",
    "    vec = max_val.squeeze()\n",
    "\n",
    "    return vec, tokens\n",
    "\n",
    "\n",
    "vec, tokens = compute_vector(text)\n",
    "print(vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b62e6c64-4a0e-4f52-8138-408bd618887f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ashe': 2.95,\n",
       " 'arthur': 2.61,\n",
       " 'tennis': 2.22,\n",
       " 'robert': 1.74,\n",
       " 'jr': 1.55,\n",
       " 'he': 1.39,\n",
       " 'founder': 1.36,\n",
       " 'doubles': 1.24,\n",
       " 'won': 1.22,\n",
       " 'slam': 1.22,\n",
       " 'died': 1.19,\n",
       " 'singles': 1.1,\n",
       " 'was': 1.07,\n",
       " 'player': 1.06,\n",
       " 'titles': 0.99,\n",
       " 'birthday': 0.99,\n",
       " 'grand': 0.93,\n",
       " 'champion': 0.93,\n",
       " 'many': 0.92,\n",
       " 'actor': 0.92,\n",
       " 'his': 0.89,\n",
       " 'death': 0.89,\n",
       " '1943': 0.86,\n",
       " 'merlin': 0.86,\n",
       " 'birth': 0.85,\n",
       " 'date': 0.84,\n",
       " 'win': 0.83,\n",
       " 'professional': 0.81,\n",
       " '1993': 0.79,\n",
       " 'early': 0.78,\n",
       " 'born': 0.77,\n",
       " 'medal': 0.75,\n",
       " 'double': 0.73,\n",
       " 'sr': 0.73,\n",
       " 'championship': 0.71,\n",
       " 'played': 0.59,\n",
       " 'celebrity': 0.59,\n",
       " 'nationality': 0.59,\n",
       " 'wimbledon': 0.53,\n",
       " 'championships': 0.5,\n",
       " 'williams': 0.5,\n",
       " 'son': 0.49,\n",
       " 'carter': 0.49,\n",
       " 'tournament': 0.44,\n",
       " 'title': 0.43,\n",
       " '1991': 0.43,\n",
       " 'champ': 0.43,\n",
       " 'edgar': 0.4,\n",
       " 'alex': 0.38,\n",
       " 'medals': 0.37,\n",
       " 'him': 0.35,\n",
       " 'career': 0.35,\n",
       " 'hale': 0.33,\n",
       " 'bob': 0.32,\n",
       " 'nathan': 0.31,\n",
       " 'players': 0.3,\n",
       " 'retired': 0.29,\n",
       " 'slams': 0.29,\n",
       " 'award': 0.26,\n",
       " 'gilbert': 0.26,\n",
       " 'had': 0.25,\n",
       " 'bruce': 0.25,\n",
       " 'single': 0.23,\n",
       " 'play': 0.23,\n",
       " 'andre': 0.22,\n",
       " 'wins': 0.21,\n",
       " 'finals': 0.2,\n",
       " 'doug': 0.2,\n",
       " 'jay': 0.18,\n",
       " 'three': 0.17,\n",
       " 'record': 0.16,\n",
       " 'winning': 0.16,\n",
       " 'seven': 0.15,\n",
       " 'fl': 0.14,\n",
       " 'rob': 0.13,\n",
       " 'father': 0.12,\n",
       " 'chess': 0.12,\n",
       " 'athlete': 0.12,\n",
       " 'july': 0.1,\n",
       " 'open': 0.1,\n",
       " 'bailey': 0.1,\n",
       " 'serena': 0.09,\n",
       " 'ali': 0.08,\n",
       " 'profile': 0.07,\n",
       " 'tiger': 0.07,\n",
       " 'held': 0.06,\n",
       " 'ellis': 0.06,\n",
       " 'golf': 0.05,\n",
       " 'haley': 0.05,\n",
       " 'name': 0.04,\n",
       " 'jul': 0.04,\n",
       " 'king': 0.03,\n",
       " 'morris': 0.03,\n",
       " 'american': 0.02,\n",
       " 'alexander': 0.02,\n",
       " 'wright': 0.02,\n",
       " '10': 0.01,\n",
       " 'charles': 0.01,\n",
       " 'artist': 0.01,\n",
       " 'israel': 0.01,\n",
       " 'davis': 0.01,\n",
       " 'hawkins': 0.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_and_map_sparse_vector(vector, tokenizer):\n",
    "    \"\"\"\n",
    "    Extracts non-zero elements from a given vector and maps these elements to their human-readable tokens using a tokenizer. The function creates and returns a sorted dictionary where keys are the tokens corresponding to non-zero elements in the vector, and values are the weights of these elements, sorted in descending order of weights.\n",
    "\n",
    "    This function is useful in NLP tasks where you need to understand the significance of different tokens based on a model's output vector. It first identifies non-zero values in the vector, maps them to tokens, and sorts them by weight for better interpretability.\n",
    "\n",
    "    Args:\n",
    "    vector (torch.Tensor): A PyTorch tensor from which to extract non-zero elements.\n",
    "    tokenizer: The tokenizer used for tokenization in the model, providing the mapping from tokens to indices.\n",
    "\n",
    "    Returns:\n",
    "    dict: A sorted dictionary mapping human-readable tokens to their corresponding non-zero weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract indices and values of non-zero elements in the vector\n",
    "    cols = vector.nonzero().squeeze().cpu().tolist()\n",
    "    weights = vector[cols].cpu().tolist()\n",
    "\n",
    "    # Map indices to tokens and create a dictionary\n",
    "    idx2token = {idx: token for token, idx in tokenizer.get_vocab().items()}\n",
    "    token_weight_dict = {idx2token[idx]: round(weight, 2) for idx, weight in zip(cols, weights)}\n",
    "\n",
    "    # Sort the dictionary by weights in descending order\n",
    "    sorted_token_weight_dict = {k: v for k, v in sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    return sorted_token_weight_dict\n",
    "\n",
    "# Usage example\n",
    "sorted_tokens = extract_and_map_sparse_vector(vec, tokenizer)\n",
    "sorted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Qdrant client setup\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import models\n",
    "\n",
    "# Define collection name\n",
    "COLLECTION_NAME = \"example_collection\"\n",
    "\n",
    "# Insert sparse vector into Qdrant collection\n",
    "point_id = 1  # Assign a unique ID for the point\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d39b075-7791-4688-88ea-af27ce2f971b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config={},\n",
    "    sparse_vectors_config={\n",
    "        \"text\": models.SparseVectorParams(\n",
    "            index=models.SparseIndexParams(\n",
    "                on_disk=False,\n",
    "            )\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a6b5da-d784-4b07-8e39-98f228f017b3",
   "metadata": {},
   "source": [
    "## insert vectors to collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b954961-de3b-424e-accb-d2f31f4a05aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdb5c9b3a8d42cabc4c26fc9740006c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import uuid\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "FILE = \"../data/data.json\"\n",
    "metadata_keys = [\"name\", \"summary\", \"url\", \"category\", \"updated_at\"]\n",
    "workplace_docs = []\n",
    "\n",
    "# load documents from source to langchain doc store\n",
    "with open(FILE, \"rt\") as f:\n",
    "    for doc in json.loads(f.read()):\n",
    "        workplace_docs.append(\n",
    "            Document(\n",
    "                page_content=doc[\"content\"],\n",
    "                metadata={k: doc.get(k) for k in metadata_keys},\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"Loaded {len(workplace_docs)} documents\")\n",
    "\n",
    "\n",
    "# split documents based on splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer=tokenizer,\n",
    "    chunk_size=512, chunk_overlap=256\n",
    ")\n",
    "docs = text_splitter.transform_documents(workplace_docs)\n",
    "\n",
    "# print(docs[0])\n",
    "# print(len(docs))\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    vec, tokens = compute_vector(doc.page_content)\n",
    "    \n",
    "    indices = vec.nonzero().numpy().flatten()\n",
    "    values = vec.detach().numpy()[indices]\n",
    "    \n",
    "    point_id = str(uuid.uuid4())\n",
    "    client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=point_id,\n",
    "                payload={key: doc.to_json()[\"kwargs\"][\"metadata\"].get(key) for key in metadata_keys},  # Add any additional payload if necessary\n",
    "                vector={\n",
    "                    \"text\": models.SparseVector(\n",
    "                        indices=indices.tolist(), values=values.tolist()\n",
    "                    )\n",
    "                },\n",
    "            )\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e55215-a01a-4f40-bf80-8ee6e08965d9",
   "metadata": {},
   "source": [
    "## find similar text via vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e928ae7-bf91-4dae-a9f9-33830b5931f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': 2.51, '##f': 2.34, 'view': 2.08, '##h': 2.05, 'views': 2.02, 'companies': 1.88, 'company': 1.65, 'businesses': 0.97, 'industry': 0.59, 'about': 0.46, 'success': 0.41, 'marketing': 0.39, 'industries': 0.37, 'corporate': 0.31, 'stock': 0.3, 'benefit': 0.29, 'association': 0.24, 'strategy': 0.23, 'market': 0.2, 'offer': 0.2, 'investment': 0.18, 'manufacturer': 0.17, 'competitor': 0.17, 'rating': 0.16, 'advertising': 0.15, 'said': 0.11, 'useful': 0.09, 'organization': 0.08, '?': 0.06, 'brand': 0.06, 'trade': 0.01, 'product': 0.01, 'website': 0.01}\n",
      "['Fy2024 Company Sales Strategy', 'New Employee Onboarding Guide', 'Intellectual Property Policy', 'Fy2024 Company Sales Strategy', 'Compensation Framework For It Teams', 'Sales Engineering Collaboration', 'Sales Organization Overview', 'Compensation Framework For It Teams', 'Code Of Conduct', 'April Work From Home Update']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Preparing a query vector\n",
    "\n",
    "query_text = \"What are companies views about WFH?\"\n",
    "query_vec, query_tokens = compute_vector(query_text)\n",
    "query_vec.shape\n",
    "\n",
    "query_expansion = extract_and_map_sparse_vector(query_vec, tokenizer)\n",
    "print(query_expansion)\n",
    "\n",
    "query_indices = query_vec.nonzero().numpy().flatten()\n",
    "query_values = query_vec.detach().numpy()[query_indices]\n",
    "\n",
    "# Searching for similar documents\n",
    "result = client.search(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query_vector=models.NamedSparseVector(\n",
    "        name=\"text\",\n",
    "        vector=models.SparseVector(\n",
    "            indices=query_indices,\n",
    "            values=query_values,\n",
    "        ),\n",
    "    ),\n",
    "    with_vectors=True,\n",
    ")\n",
    "\n",
    "print([res.payload[\"name\"] for res in result])\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b0f89-1e30-4cba-8e13-46f9342f749d",
   "metadata": {},
   "source": [
    "# Ask llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a7c2d-7693-45b7-aa91-23d5b01fc776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
