{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0a9802-1b39-4ccb-9ebd-012bcab4fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import ujson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240e832-154a-44ac-a6ec-7da0d9c96e7b",
   "metadata": {},
   "source": [
    "# connect with db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63311997-51f8-472e-9ac1-9b87f2940ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SQLITE_DB_PATH = os.getenv(\"SQLITE_DB_PATH\", f\"{os.path.expanduser('~')}/.watchtower/watchtower.db\")\n",
    "\n",
    "# connect with db\n",
    "conn = sqlite3.connect(SQLITE_DB_PATH)\n",
    "\n",
    "# fetch all literature records as objects\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM literatures\")\n",
    "literature_records = cursor.fetchall()\n",
    "headers = [description[0] for description in cursor.description]\n",
    "records = [dict(zip(headers, record)) for record in literature_records]\n",
    "for record in records:\n",
    "    record[\"content\"] = ujson.loads(record[\"content\"])\n",
    "# close connection\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "019d34af-b077-4ddc-b207-7ff4cf90fe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records:28\n",
      "697\n",
      "A neural probabilistic language model : 8\n",
      "A unified architecture for natural language processing: Deep neural networks with multitask learning : 4\n",
      "A scalable hierarchical distributed language model : 4\n",
      "Continuous space language models : 4\n",
      "Long short-term memory : 4\n",
      "Hierarchical probabilistic neural network language model : 3\n",
      "Recurrent neural network based language model : 3\n",
      "Strategies for Training Large Scale Neural Network Language Models : 3\n",
      "Statistical Language Models based on Neural Networks : 3\n",
      "Distributed representations of words and phrases and their compositionality : 3\n",
      "Effective self-training for parsing : 2\n",
      "Connectionist language modeling for large vocabulary continuous speech recognition : 2\n",
      "Class-based n-gram models of natural language : 2\n",
      "Three new graphical models for statistical language modelling : 2\n",
      "Distributional clustering of english words : 2\n",
      "From frequency to meaning: Vector space models of semantics : 2\n",
      "Indexing by latent semantic analysis : 2\n",
      "Learning distributed representations of concepts : 2\n",
      "Adaptive subgradient methods for online learning and stochastic optimization : 2\n",
      "Semeval-2012 task 2: Measuring degrees of relational similarity : 2\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "citation_dict = defaultdict(list)\n",
    "print(f\"records:{len(records)}\")\n",
    "for record in records:\n",
    "    # record[\"content\"] = ujson.loads(record[\"content\"])\n",
    "    # print(f\"title:{record['title']}\")\n",
    "    # print(record[\"content\"].keys())\n",
    "    record_content= record[\"content\"]\n",
    "    citation_dict[record_content['bibliography'][\"title\"]] = []\n",
    "    for key, citation in record_content[\"citations\"].items():\n",
    "        # print(citation[\"title\"])\n",
    "        citation_dict[citation[\"title\"]].append(record[\"title\"])\n",
    "print(len(citation_dict))\n",
    "\n",
    "# sort citation_dict by citation count\n",
    "citation_dict = dict(sorted(citation_dict.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "\n",
    "for key, value in list(citation_dict.items())[:20]:\n",
    "    print(f\"{key} : {len(value)}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60bfe4c2ea5ce84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A neural probabilistic language model: 0.001706\n",
      "Continuous space language models: 0.001588\n",
      "A scalable hierarchical distributed language model: 0.001557\n",
      "Strategies for Training Large Scale Neural Network Language Models: 0.001551\n",
      "Long short-term memory: 0.00155\n",
      "Indexing by latent semantic analysis: 0.001548\n",
      "Learning distributed representations of concepts: 0.001548\n",
      "A unified architecture for natural language processing: Deep neural networks with multitask learning: 0.00154\n",
      "Hierarchical probabilistic neural network language model: 0.001535\n",
      "Recurrent neural network based language model: 0.001527\n",
      "Statistical Language Models based on Neural Networks: 0.001523\n",
      "Distributed representations of words and phrases and their compositionality: 0.001523\n",
      "Distributional clustering of english words: 0.00152\n",
      "Learning representations by backpropagating errors: 0.001508\n",
      "Comparative evaluation of index languages, Part II; Results: 0.001506\n",
      "The effectiveness of automatically-generated weights and links: 0.001506\n",
      "Interactive document storage and retrieval systems -design concepts: 0.001506\n",
      "Factors Determining the Performance of Indexing Systems: 0.001506\n",
      "An Operational Interactive Retrieval System: 0.001506\n",
      "Report of an Information Science Index Languages Test: 0.001506\n",
      "Information Retrieval Systems: Characteristics, Testing and Evaluation: 0.001506\n",
      "Automatic Information Organization and Retrieval: 0.001506\n",
      "Computer evaluation of indexing and text processing: 0.001506\n",
      "Automatic Keyword Classification for Information Retrieval: 0.001506\n",
      "Distribution of indexing terms for maximum efficiency of information transmission: 0.001506\n",
      "The anatomy of a large-scale hypertextual Web search engine: 0.001506\n",
      "Document understanding conference 2002: 0.001506\n",
      "Domain-specific keyphrase extraction: 0.001506\n",
      "Cohesion in English: 0.001506\n",
      "A model for natural language semantics: 0.001506\n"
     ]
    }
   ],
   "source": [
    "# generate citation graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# draw graph with node size proportional to centrality\n",
    "def draw_graph(G, pos, node_size, title):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=node_size, font_size=10, font_color=\"black\", font_weight=\"bold\", node_color=\"skyblue\", edge_color=\"gray\", linewidths=1, width=1, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "G = nx.DiGraph()\n",
    "for key, citations in citation_dict.items():\n",
    "    # if len(citations)<2:\n",
    "    #     break\n",
    "    for citation in citations:\n",
    "        # print(f\"{v}=>{key}\")\n",
    "        G.add_edge(citation, key)\n",
    "\n",
    "# calculate pagerank\n",
    "paper_pagerank = nx.pagerank(G)\n",
    "# print(pagerank)\n",
    "\n",
    "paper_pagerank = dict(sorted(paper_pagerank.items(), key=lambda x: x[1], reverse=True))\n",
    "# print(pagerank)\n",
    "for author, importance in list(paper_pagerank.items())[:30]:\n",
    "    print(f\"{author}: {round(importance,6)}\")\n",
    "\n",
    "\n",
    "# # draw graph\n",
    "# pos = nx.spring_layout(G)\n",
    "# node_size = [pagerank[node] * 10000 for node in G]\n",
    "# draw_graph(G, pos, node_size, \"Citation Graph\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba444b28-d1d4-44a7-b1bd-9e7f7b187094",
   "metadata": {},
   "source": [
    "## Author rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db165952-4f4e-48c7-a60a-07e216a01b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_citation_dict = defaultdict(list)\n",
    "for record in records:\n",
    "    citing_authors = [(auth[\"person_name\"].get(\"first_name\", \" \")[0], auth[\"person_name\"][\"surname\"]) for auth in record[\"content\"][\"bibliography\"][\"authors\"]]\n",
    "    # print(citing_authors)\n",
    "    # citation_authors =\n",
    "    record_content= record[\"content\"]\n",
    "    # citation_dict[record_content['bibliography'][\"title\"]] = []\n",
    "    for key, citation in record_content[\"citations\"].items():\n",
    "        # print(citation)\n",
    "        cited_authors = [(auth[\"person_name\"].get(\"first_name\", \" \"), auth[\"person_name\"][\"surname\"]) for auth in citation[\"authors\"]]\n",
    "        cited_authors = [(f[0] if f else \"\", l if l else \"\") for f,l in cited_authors]\n",
    "        # print(cited_authors)\n",
    "        for cited_author in cited_authors:\n",
    "            # print(cited_author)\n",
    "            author_citation_dict[cited_author].extend(citing_authors)\n",
    "            # break\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0ac11-09f4-409d-8842-6017da640901",
   "metadata": {},
   "source": [
    "## Citation count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ccc80cc-0656-458d-a288-7521f2700351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Y', 'Bengio') : 223\n",
      "('T', 'Mikolov') : 112\n",
      "('G', 'Hinton') : 97\n",
      "('I', 'Sutskever') : 76\n",
      "('C', 'Manning') : 68\n",
      "('R', 'Socher') : 68\n",
      "('J', 'Weston') : 65\n",
      "('H', 'Schwenk') : 57\n",
      "('H', 'Chen') : 52\n",
      "('N', 'Zhang') : 52\n",
      "('Q', 'Le') : 50\n",
      "('J', 'Dean') : 47\n",
      "('A', 'Ng') : 46\n",
      "('K', 'Cho') : 41\n",
      "('L', 'Burget') : 40\n",
      "('K', 'Liu') : 40\n",
      "('A', 'Bordes') : 39\n",
      "('R', 'Pascanu') : 38\n",
      "('R', 'Collobert') : 37\n",
      "('O', 'Vinyals') : 36\n"
     ]
    }
   ],
   "source": [
    "author_citation_dict = dict(sorted(author_citation_dict.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "\n",
    "for key, value in list(author_citation_dict.items())[:20]:\n",
    "    print(f\"{key} : {len(value)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57708b3e-9c41-4d87-88be-20e8b8afd01d",
   "metadata": {},
   "source": [
    "## author pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52cbba6e-1597-42f7-9869-c3db1906efc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Y', 'Bengio'): 0.000827\n",
      "('G', 'Hinton'): 0.000815\n",
      "('J', 'Weston'): 0.000792\n",
      "('C', 'Manning'): 0.000766\n",
      "('H', 'Schwenk'): 0.000754\n",
      "('J', 'Dean'): 0.000745\n",
      "('R', 'Socher'): 0.000744\n",
      "('R', 'Collobert'): 0.000743\n",
      "('T', 'Mikolov'): 0.00074\n",
      "('I', 'Sutskever'): 0.000738\n",
      "('A', 'Bordes'): 0.000734\n",
      "('A', 'Ng'): 0.000717\n",
      "('P', 'Vincent'): 0.000713\n",
      "('R', 'Ducharme'): 0.000712\n",
      "('G', 'Corrado'): 0.000711\n",
      "('X', 'Glorot'): 0.000706\n",
      "('P', 'Turney'): 0.000705\n",
      "('K', 'Chen'): 0.000705\n",
      "('J', 'Turian'): 0.000701\n",
      "('R', 'Harshman'): 0.000683\n",
      "('R', 'Salakhutdinov'): 0.000679\n",
      "('C', 'Xiong'): 0.000675\n",
      "('S', 'Bengio'): 0.000674\n",
      "('M', 'Zhang'): 0.000673\n",
      "('N', 'Usunier'): 0.000666\n",
      "('C', 'Lin'): 0.000665\n",
      "('L', 'Burget'): 0.00066\n",
      "('L', 'Ratinov'): 0.00066\n",
      "('A', 'Mnih'): 0.00066\n",
      "('J', 'Schmidhuber'): 0.000655\n"
     ]
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "for key, citations in author_citation_dict.items():\n",
    "    # if len(citations)<30:\n",
    "    #     break\n",
    "    for citation in citations:\n",
    "        # print(f\"{v}=>{key}\")\n",
    "        G.add_edge(citation, key)\n",
    "\n",
    "# calculate pagerank\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "\n",
    "pagerank = dict(sorted(pagerank.items(), key=lambda x: x[1], reverse=True))\n",
    "# print(pagerank)\n",
    "for author, importance in list(pagerank.items())[:30]:\n",
    "    print(f\"{author}: {round(importance, 6)}\")\n",
    "\n",
    "# # draw graph\n",
    "# pos = nx.spring_layout(G)\n",
    "# node_size = [pagerank[node] * 10000 for node in G]\n",
    "# draw_graph(G, pos, node_size, \"Author Citation Graph\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
